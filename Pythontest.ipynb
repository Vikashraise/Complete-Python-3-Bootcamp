{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0/707IHlmiOVRQFH27WsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikashraise/Complete-Python-3-Bootcamp/blob/master/Pythontest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "id": "FL5ub3hqME7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73e1ca8-0f0f-453e-f85a-78f3d0307092"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def text_similarity(text1, text2):\n",
        "    # Load spaCy model with Word2Vec vectors\n",
        "    nlp = spacy.load(\"en_core_web_md\") ##I am using spaCy model named en_core_web_md, which includes pre-trained Word2Vec word embeddings for English.\n",
        "\n",
        "    # Get spaCy Doc objects for each text\n",
        "    doc1 = nlp(text1)\n",
        "    doc2 = nlp(text2)\n",
        "\n",
        "    # Compute cosine similarity between document vectors\n",
        "    similarity_score = doc1.similarity(doc2)\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage:\n",
        "text1 = \"This is a good country.\"\n",
        "text2 = \"I just had a fight.\"\n",
        "similarity_score = text_similarity(text1, text2)\n",
        "print(f\"Cosine Similarity: {similarity_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYfczuJMXd7",
        "outputId": "d6d2ad50-34f2-4a73-83b4-ed6a13972d35"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.6616039282022231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\") ##I am using spaCy model named en_core_web_md, which includes pre-trained Word2Vec word embeddings for English.\n",
        "\n",
        "    # Get spaCy Doc objects for each text\n",
        "doc1 = nlp(text1)\n",
        "doc2 = nlp(text2)\n",
        "print(doc1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6SCaXJUJPF7",
        "outputId": "b79193ed-6344-4d36-b96d-30c0b232943f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first document about text similarity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makengrams(text, n): #Making a function named makengrams, with arguments as text and n which is refers to 1 for uni,2 for 3 etc\n",
        "\n",
        "    words = text.split() # TO CONVERT IT INTO A LIST OF WORDS\n",
        "    ngrams = [words[i:i + n] for i in range(len(words) - n + 1)] #FOR LOOP TO ITERATE OVER THE INDICES\n",
        "    return ngrams\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"Hello Welcome to my Code.\"\n",
        "unigram=makengrams(input_text, 1)\n",
        "bigrams = makengrams(input_text, 2)\n",
        "trigrams = makengrams(input_text, 3)"
      ],
      "metadata": {
        "id": "4TTmWD0iMh0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcwv0DNYQhr2",
        "outputId": "3b1f721e-cc0a-4874-eea8-b6823f543583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello'], ['Welcome'], ['to'], ['my'], ['Code.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Hello welcome to my code.\"\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRNPtw-NQomI",
        "outputId": "e366e055-5d27-4e8d-dfd5-715f0545d5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'welcome', 'to', 'my', 'code.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk     #I will be using Nltk library for this task,so I have imported the necessary libraries\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "EZgRGlJ4Ui-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GapcKLAwUhiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SULFmcL2VVGF",
        "outputId": "8fc5adc0-c318-427f-a92b-520af6883c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams, word_tokenize\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Generate unigrams, bigrams, and ngrams for the given text.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): Input text.\n",
        "    - n (int): Size of ngrams (default is 2 for bigrams).\n",
        "\n",
        "    Returns:\n",
        "    - Tuple: Tuple containing lists of unigrams, bigrams, and ngrams.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Unigrams\n",
        "    unigrams = tokens\n",
        "\n",
        "    # Bigrams\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "\n",
        "    # Ngrams\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "    return unigrams, bigrams, ngrams_list\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is an example sentence.\"\n",
        "unigrams, bigrams, ngrams_list = generate_ngrams(text, n=3)\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", ngrams_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BHMGlNRVXa0",
        "outputId": "4141a274-bdd4-4b49-cf69-17fadc65cfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: ['This', 'is', 'an', 'example', 'sentence', '.']\n",
            "Bigrams: [('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'sentence'), ('sentence', '.')]\n",
            "Trigrams: [('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'sentence'), ('example', 'sentence', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makengrams(text, n): #Making a function named makengrams, with arguments as text and n which is refers to 1 for uni,2 for 3 etc\n",
        "\n",
        "    words = text.split() # TO CONVERT IT INTO A LIST OF WORDS\n",
        "    ngrams = [words[i:i + n] for i in range(len(words) - n + 1)] #FOR LOOP TO ITERATE OVER THE INDICES\n",
        "    return ngrams\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"Hello Welcome to my Code.\"\n",
        "unigram=makengrams(input_text, 1)\n",
        "bigrams = makengrams(input_text, 2)\n",
        "trigrams = makengrams(input_text, 3)\n",
        "print(\"Uni-grams:\", unigram)\n",
        "print(\"Bi-grams:\", bigrams)\n",
        "print(\"Tri-grams:\", trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-hrX1cBemrm",
        "outputId": "15dfc2d3-a5fc-46e8-d1e0-373e2d766760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uni-grams: [['Hello'], ['Welcome'], ['to'], ['my'], ['Code.']]\n",
            "Bi-grams: [['Hello', 'Welcome'], ['Welcome', 'to'], ['to', 'my'], ['my', 'Code.']]\n",
            "Tri-grams: [['Hello', 'Welcome', 'to'], ['Welcome', 'to', 'my'], ['to', 'my', 'Code.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation"
      ],
      "metadata": {
        "id": "st7KXxE3sRzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tawYcSRKrwdp",
        "outputId": "06e3f876-7e4b-4332-e04b-60564302d793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Hello Welcome to my code\"\n",
        "words=text.split()\n",
        "words[1:3]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVagsKfYryFP",
        "outputId": "19c16307-85b3-468b-e2cf-1ba0e98d3901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Welcome', 'to']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list=[\"hello\",\"This\",\"Is\",\"My\",\"Code\"]"
      ],
      "metadata": {
        "id": "ZR3q4omXHh0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_list[0:2]"
      ],
      "metadata": {
        "id": "l3s26p6pslBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0c6743-ad7d-4a67-b553-a0d475613946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'This']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2)  Q2# detect email address and phone number, name and address from a pdf file (Use your own resume if required or download file from internet)"
      ],
      "metadata": {
        "id": "PuDtQTTIMCOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efeLawhuyf7d",
        "outputId": "d4c8bc08-96fd-4511-b6d7-806c5fe5f142"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/284.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/284.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "h7zxdrvKMLu5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Corrected path using double backslashes or a raw string\n",
        "mypdf = PdfReader(\"/content/Resumedraft.pdf\")  # or r\"C:\\Users\\HP\\Desktop\\docs\\Resumedraft.pdf\"\n",
        "\n",
        "print(f\"There are {len(mypdf.pages)} Pages\")\n",
        "\n",
        "page = mypdf.pages[0]\n",
        "print(page.extract_text())\n",
        "\n",
        "for i in range(len(mypdf.pages)):\n",
        "    page = mypdf.pages[i]\n",
        "    print(page.extract_text())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrk5S790MR-e",
        "outputId": "42fccda2-69e7-48b9-83e6-bf09bcf6ea68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 Pages\n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n",
            "E-mail Spam Classifier | Natural Language Processing | Self Project                                                                       \n",
            " Anticipated and preprocessed the data by stop words removal, tokenization, stemming and vectorization  \n",
            " Applied Naïve Bayes classifier  and evaluated the model with  precision, recall  and F1 score as the evaluation metric  \n",
            "House Price Prediction | Machine Learning | Self Project                                                                                             \n",
            " Applied Linear Regression  algorithm to predict best selling price of house  with  MAPE  and RMSE as the evalu ation metric  \n",
            " Implemented feature engineering techniques like one hot encoding and  binning  to achieve best  MAPE score of 0.17  \n",
            "Heart Attack Analysis and Prediction | Machine learning | Guided | Udemy                                                   \n",
            " Performed  univariate  and bivariate  analysis using Facet Grid , Count Plot , Pair Plot , Swarm Plot , Box Plot  and Heat Map  \n",
            " Used Logistic Regression , Support Vector Classifier  and Random Forest  to get best accuracy of 90.3%  and AUC  of 0.93  \n",
            "Introduction to Data Analysis using MS Excel  \n",
            " Used functions such  as IF, VLOOKUP and offset on complex data, created pivot tables to perform advanced data analysis  \n",
            "POSITIONS OF RESPONSIBILITY  \n",
            "Manager  | Good Girls and Boys (GGB) | Student Chapter| Social Outreach | VIT Vellore                                    \n",
            " Spearheaded a team of 17 members to provide basic education to 15 +  underprivileged  kids of migrant workers  \n",
            " Coordinated  with higher authorities and organised fundraiser  to gather funds of 20,000  INR for educational purpose s \n",
            " Arranged  classroom and provided stationaries, resulting in increased willingness among k ids to participate  \n",
            "Student Member | Indian Concrete Institute (ICI) | Student Chapter | VIT Vellore                                                 \n",
            " Coordinated  with 32 +  members to organize events like BUILD -A-THON , workshops and site visit  \n",
            " Assisted  on smooth conduct of National Conference  on Special concrete sponsored by HIL, HEICO, Ultratech   \n",
            "TECHNICAL  SKILLS  \n",
            " Software’s and Visualization Tools: Python | SQL | MS - Excel | | Tableau  \n",
            " Strong foundation in statistics and data visualization tools; Experience with machine learning  algorithms ; \n",
            " Ability to work with large datasets  and extract insights; Strong problem solving and analytical skills  \n",
            "CERTIFICATIONS, KEY COURSES AND  EXTRACURRICULAR ACTIVITIES  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Certifications   Programming for everybody ( Python ), 4-week course on Coursera by Michigan University    \n",
            " Machine learning  for Beginners certification, 64+ hours course by Analytics Vidhya       \n",
            " IBM Data Science Professional Course  which included | What is Data Science (3 weeks) |Tools for \n",
            "data Science (4 weeks) | Data Science Methodology (3 weeks) | Python for Data Science ( 5 weeks)    \n",
            " Financial Risk Analytics  by Great Learning     |   Data Visualization with Tableau  by Great Learning  \n",
            " Getting started with Artificial Neural Network  by Analytics Vidhya      \n",
            "Key courses   Completed a MTech course on Applied  Statistics  CE 605| IIT Bombay                                                  \n",
            " Completed a BTech Course on Statistics for Engineer  MAT 2001|VIT Vellore                                                                                         \n",
            " \n",
            " \n",
            " \n",
            "Extracurriculars   Achieved All India Rank of 564 IN GATE 2021 among 0.11  million students                                                                                                           \n",
            "    Achieved A grade  in IDEATHON  conducted by Department of Mathematics, VIT Vellore                              \n",
            "    Participated in 10 km  marathon organized by Sports Club, VIT Vellore                                                \n",
            " Teaching member of Social outreach club which provides education to underprivileged ki ds        \n",
            " Participated in VIT Premier Cricket league, among 8 /106  teams to qualify for quarter finals            \n",
            " \n",
            " Examination  University  Institute  Year  CPI \n",
            "Post Graduation  IIT Bombay  IIT Bombay  2023  8.97  \n",
            "Graduation  VIT Vellore  VIT Vellore  2020  8.99  \n",
            "An aspiring data scientist with Mater’s Degree from IIT Bombay, having a strong foundation in statistical methods, \n",
            "programming, and machine learning. I am passionate about using data to solve real -world problems using my \n",
            "analytical and problem solving skills. With strong ability to work with large datasets and extract insights, I am a \n",
            "motivated and  analytical thinker with a keen interest in exploring the latest technologies and techniques in the field \n",
            "of data science.  \n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n",
            "E-mail Spam Classifier | Natural Language Processing | Self Project                                                                       \n",
            " Anticipated and preprocessed the data by stop words removal, tokenization, stemming and vectorization  \n",
            " Applied Naïve Bayes classifier  and evaluated the model with  precision, recall  and F1 score as the evaluation metric  \n",
            "House Price Prediction | Machine Learning | Self Project                                                                                             \n",
            " Applied Linear Regression  algorithm to predict best selling price of house  with  MAPE  and RMSE as the evalu ation metric  \n",
            " Implemented feature engineering techniques like one hot encoding and  binning  to achieve best  MAPE score of 0.17  \n",
            "Heart Attack Analysis and Prediction | Machine learning | Guided | Udemy                                                   \n",
            " Performed  univariate  and bivariate  analysis using Facet Grid , Count Plot , Pair Plot , Swarm Plot , Box Plot  and Heat Map  \n",
            " Used Logistic Regression , Support Vector Classifier  and Random Forest  to get best accuracy of 90.3%  and AUC  of 0.93  \n",
            "Introduction to Data Analysis using MS Excel  \n",
            " Used functions such  as IF, VLOOKUP and offset on complex data, created pivot tables to perform advanced data analysis  \n",
            "POSITIONS OF RESPONSIBILITY  \n",
            "Manager  | Good Girls and Boys (GGB) | Student Chapter| Social Outreach | VIT Vellore                                    \n",
            " Spearheaded a team of 17 members to provide basic education to 15 +  underprivileged  kids of migrant workers  \n",
            " Coordinated  with higher authorities and organised fundraiser  to gather funds of 20,000  INR for educational purpose s \n",
            " Arranged  classroom and provided stationaries, resulting in increased willingness among k ids to participate  \n",
            "Student Member | Indian Concrete Institute (ICI) | Student Chapter | VIT Vellore                                                 \n",
            " Coordinated  with 32 +  members to organize events like BUILD -A-THON , workshops and site visit  \n",
            " Assisted  on smooth conduct of National Conference  on Special concrete sponsored by HIL, HEICO, Ultratech   \n",
            "TECHNICAL  SKILLS  \n",
            " Software’s and Visualization Tools: Python | SQL | MS - Excel | | Tableau  \n",
            " Strong foundation in statistics and data visualization tools; Experience with machine learning  algorithms ; \n",
            " Ability to work with large datasets  and extract insights; Strong problem solving and analytical skills  \n",
            "CERTIFICATIONS, KEY COURSES AND  EXTRACURRICULAR ACTIVITIES  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Certifications   Programming for everybody ( Python ), 4-week course on Coursera by Michigan University    \n",
            " Machine learning  for Beginners certification, 64+ hours course by Analytics Vidhya       \n",
            " IBM Data Science Professional Course  which included | What is Data Science (3 weeks) |Tools for \n",
            "data Science (4 weeks) | Data Science Methodology (3 weeks) | Python for Data Science ( 5 weeks)    \n",
            " Financial Risk Analytics  by Great Learning     |   Data Visualization with Tableau  by Great Learning  \n",
            " Getting started with Artificial Neural Network  by Analytics Vidhya      \n",
            "Key courses   Completed a MTech course on Applied  Statistics  CE 605| IIT Bombay                                                  \n",
            " Completed a BTech Course on Statistics for Engineer  MAT 2001|VIT Vellore                                                                                         \n",
            " \n",
            " \n",
            " \n",
            "Extracurriculars   Achieved All India Rank of 564 IN GATE 2021 among 0.11  million students                                                                                                           \n",
            "    Achieved A grade  in IDEATHON  conducted by Department of Mathematics, VIT Vellore                              \n",
            "    Participated in 10 km  marathon organized by Sports Club, VIT Vellore                                                \n",
            " Teaching member of Social outreach club which provides education to underprivileged ki ds        \n",
            " Participated in VIT Premier Cricket league, among 8 /106  teams to qualify for quarter finals            \n",
            " \n",
            " Examination  University  Institute  Year  CPI \n",
            "Post Graduation  IIT Bombay  IIT Bombay  2023  8.97  \n",
            "Graduation  VIT Vellore  VIT Vellore  2020  8.99  \n",
            "An aspiring data scientist with Mater’s Degree from IIT Bombay, having a strong foundation in statistical methods, \n",
            "programming, and machine learning. I am passionate about using data to solve real -world problems using my \n",
            "analytical and problem solving skills. With strong ability to work with large datasets and extract insights, I am a \n",
            "motivated and  analytical thinker with a keen interest in exploring the latest technologies and techniques in the field \n",
            "of data science.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Corrected path using double backslashes or a raw string\n",
        "mypdf = PdfReader(\"/content/Resumedraft.pdf\")  # or r\"C:\\Users\\HP\\Desktop\\docs\\Resumedraft.pdf\"\n",
        "\n",
        "print(f\"There are {len(mypdf.pages)} Pages\")\n",
        "\n",
        "# Extract and print the first few lines from the first page\n",
        "page = mypdf.pages[0]\n",
        "num_lines_to_extract = 5  # Change this value to the desired number of lines\n",
        "text_lines = page.extract_text().split('\\n')[:num_lines_to_extract]\n",
        "print('\\n'.join(text_lines))\n",
        "\n",
        "# Extract and print the first few lines from each page\n",
        "for i in range(len(mypdf.pages)):\n",
        "    page = mypdf.pages[i]\n",
        "    text_lines = page.extract_text().split('\\n')[:num_lines_to_extract]\n",
        "    print('\\n'.join(text_lines))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTv_VozMUvu",
        "outputId": "51bf19f6-4c3e-4eb4-f650-1f86a9a39c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 Pages\n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "VYbga16YWf4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected path using double backslashes or a raw string\n",
        "mypdf = PdfReader(\"/content/Vikashdraftresume.pdf\")  # or r\"C:\\Users\\HP\\Desktop\\docs\\Resumedraft.pdf\"\n",
        "\n",
        "print(f\"There are {len(mypdf.pages)} Pages\")\n",
        "\n",
        "page = mypdf.pages[0]\n",
        "print(page.extract_text())\n",
        "\n",
        "for i in range(len(mypdf.pages)):\n",
        "    page = mypdf.pages[i]\n",
        "    print(page.extract_text())"
      ],
      "metadata": {
        "id": "laVXRf2ENh8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ac077e-3061-4da1-e1f0-54883d98609f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 Pages\n",
            "Name – Vikash Rai  \n",
            "Contact No – 9629178894  \n",
            "Email id – vikashraise@gmail.com  \n",
            "Address – Flat 1405, F2 block, Plot 11, Golf City, Sector 75, Noida ,201301,Uttar Pradesh  \n",
            "EDUCATION  \n",
            "M.Tech – IIT Bombay,[2021 -2023 ], Geotechnical Engineering , CGPA -8.97 \n",
            "B.Tech – VIT VELLORE, [ 2016 -2020] ,Civil Engineering, CGPA – 8.99  \n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved  final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n",
            "E-mail Spam Classifier | Natural Language Processing | Self Project                                                                       \n",
            " Anticipated and preprocessed the data by stop words removal, tokenization, stemming and vectorization  \n",
            " Applied Naïve Bayes classifier  and evaluated the model with  precision, recall  and F1 score as the evaluation \n",
            "metric  \n",
            "House Price Prediction | Machine Learning | Self Proje ct                                                                                             \n",
            " Applied Linear Regression  algorithm to predict best selling price of house with MAPE  and RMSE as the \n",
            "evaluation metric  \n",
            " Implemented feature engineering techniques  like one hot encoding and binning to achieve best  MAPE score \n",
            "of 0.17  \n",
            "Heart Attack Analysis and Prediction | Machine learning | Guided | Udemy                                                   \n",
            " Performed  univariate  and bivariate  analysis using Facet Grid , Count Plot , Pair Plot , Swarm Plot , Box Plot  \n",
            "and Heat Map  \n",
            " Used Logistic Regression , Support Vector Classifier  and Random Forest  to get best accuracy of 90.3%  and \n",
            "AUC of 0.93  \n",
            "Introduction to Data Analysis using MS Excel  \n",
            " Used functions such as IF, VLOOKUP and offset on complex data, created pivot tables to perform advanced \n",
            "data analysis  \n",
            "POSITIONS OF RESPONSIBILITY  \n",
            "Manager  | Good Girls and Boys (GGB) | Student Chapter| Social Outreach | VIT Vellore                                    \n",
            " Spearheaded a team of 17 membe rs to provide basic education to 15 +  underprivileged  kids of migrant \n",
            "workers  \n",
            " Coordinated  with higher authorities and organised fundraiser  to gather funds of 20,000  INR for educational \n",
            "purposes  \n",
            "Student Member | Indian Concrete Institute (ICI) | Student Ch apter | VIT Vellore                                                 \n",
            " Coordinated  with 32 +  members to organize events like BUILD -A-THON , workshops and site visit  \n",
            " Assisted  on smooth conduct of National Conference  on Special concrete sponsored by HIL, HEICO, \n",
            "Ultratech  \n",
            " \n",
            "TECHNICAL SKILLS  \n",
            " Software’s and Visualization Tools: Python | SQL | MS - Excel | | Tableau  \n",
            " Strong foundation in statistics and data visualization tools; Experience with machine learning algorithms;  \n",
            " Ability to work with large datasets and extract insights; Strong problem solving and analytical skills  \n",
            " \n",
            "CERTIFICATIONS, KEY COURSES AND EXTRACURRICULAR ACTIVITIES  \n",
            "Certifications  \n",
            "•Programming for everybody (Python), 4 -week course on Coursera by Michigan University    \n",
            "•Machine learning for Beginne rs certification, 64+ hours course by Analytics Vidhya       \n",
            "•IBM Data Science Professional Course which included | What is Data Science (3 weeks) |Tools for data Science \n",
            "(4 weeks) | Data Science Methodology (3 weeks) | Python for Data Science (5 weeks)    \n",
            "Name – Vikash Rai  \n",
            "Contact No – 9629178894  \n",
            "Email id – vikashraise@gmail.com  \n",
            "Address – Flat 1405, F2 block, Plot 11, Golf City, Sector 75, Noida ,201301,Uttar Pradesh  \n",
            "EDUCATION  \n",
            "M.Tech – IIT Bombay,[2021 -2023 ], Geotechnical Engineering , CGPA -8.97 \n",
            "B.Tech – VIT VELLORE, [ 2016 -2020] ,Civil Engineering, CGPA – 8.99  \n",
            " \n",
            "KEY PROJECTS  \n",
            "Credit Card Fraud Detection  \n",
            " Performed exploratory data analysis  and used techniques like SMOTE  to handle the imbalanced data set  \n",
            " Achieved  final ROC  AUC  score of 0.87  by employing Logistic Regression , KNN , and Decision Tree  classifiers  \n",
            "E-mail Spam Classifier | Natural Language Processing | Self Project                                                                       \n",
            " Anticipated and preprocessed the data by stop words removal, tokenization, stemming and vectorization  \n",
            " Applied Naïve Bayes classifier  and evaluated the model with  precision, recall  and F1 score as the evaluation \n",
            "metric  \n",
            "House Price Prediction | Machine Learning | Self Proje ct                                                                                             \n",
            " Applied Linear Regression  algorithm to predict best selling price of house with MAPE  and RMSE as the \n",
            "evaluation metric  \n",
            " Implemented feature engineering techniques  like one hot encoding and binning to achieve best  MAPE score \n",
            "of 0.17  \n",
            "Heart Attack Analysis and Prediction | Machine learning | Guided | Udemy                                                   \n",
            " Performed  univariate  and bivariate  analysis using Facet Grid , Count Plot , Pair Plot , Swarm Plot , Box Plot  \n",
            "and Heat Map  \n",
            " Used Logistic Regression , Support Vector Classifier  and Random Forest  to get best accuracy of 90.3%  and \n",
            "AUC of 0.93  \n",
            "Introduction to Data Analysis using MS Excel  \n",
            " Used functions such as IF, VLOOKUP and offset on complex data, created pivot tables to perform advanced \n",
            "data analysis  \n",
            "POSITIONS OF RESPONSIBILITY  \n",
            "Manager  | Good Girls and Boys (GGB) | Student Chapter| Social Outreach | VIT Vellore                                    \n",
            " Spearheaded a team of 17 membe rs to provide basic education to 15 +  underprivileged  kids of migrant \n",
            "workers  \n",
            " Coordinated  with higher authorities and organised fundraiser  to gather funds of 20,000  INR for educational \n",
            "purposes  \n",
            "Student Member | Indian Concrete Institute (ICI) | Student Ch apter | VIT Vellore                                                 \n",
            " Coordinated  with 32 +  members to organize events like BUILD -A-THON , workshops and site visit  \n",
            " Assisted  on smooth conduct of National Conference  on Special concrete sponsored by HIL, HEICO, \n",
            "Ultratech  \n",
            " \n",
            "TECHNICAL SKILLS  \n",
            " Software’s and Visualization Tools: Python | SQL | MS - Excel | | Tableau  \n",
            " Strong foundation in statistics and data visualization tools; Experience with machine learning algorithms;  \n",
            " Ability to work with large datasets and extract insights; Strong problem solving and analytical skills  \n",
            " \n",
            "CERTIFICATIONS, KEY COURSES AND EXTRACURRICULAR ACTIVITIES  \n",
            "Certifications  \n",
            "•Programming for everybody (Python), 4 -week course on Coursera by Michigan University    \n",
            "•Machine learning for Beginne rs certification, 64+ hours course by Analytics Vidhya       \n",
            "•IBM Data Science Professional Course which included | What is Data Science (3 weeks) |Tools for data Science \n",
            "(4 weeks) | Data Science Methodology (3 weeks) | Python for Data Science (5 weeks)    \n",
            "•Financial Risk Analytics by Great Learning     |   Data Visualization with Tableau by Great Learning  \n",
            "•Getting started with Artificial Neural Network by Analytics Vidhya      \n",
            "Key courses  \n",
            "•Completed a MTech course on Applied Statistics CE 605| IIT Bombay                                                  \n",
            "•Completed a BTech Course on Statistics for Engineer MAT 2001|VIT Vellore                                                                                         \n",
            "Extracurriculars  \n",
            "•Achieved All India Rank of 564 IN GATE 2021 among 0.11 million students                                                                                                          \n",
            "• Achieved A grade in IDEATHON conducted by Department of Mathematics, VIT Vel lore                             \n",
            "•Participated in 10 km marathon organized by Sports Club, VIT Vellore                                                \n",
            "•Teaching member of Social outreach club which provides education to underprivileged kids        \n",
            "•Particip ated in VIT Premier Cricket league, among 8 /106 teams to qualify for quarter finals            \n",
            " \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import re\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_information(text):\n",
        "    name_pattern = re.compile(r'\\bName-(.+)', re.IGNORECASE)\n",
        "    email_pattern = re.compile(r'\\bEmail id- (.+)', re.IGNORECASE)\n",
        "    address_pattern = re.compile(r'\\bAddress- (.+)', re.IGNORECASE)\n",
        "\n",
        "    # Apply patterns to the provided text\n",
        "    name_match = name_pattern.search(text)\n",
        "    email_match = email_pattern.search(text)\n",
        "    address_match = address_pattern.search(text)\n",
        "\n",
        "    # Extract information if patterns are found\n",
        "    name = name_match.group(1).strip() if name_match else None\n",
        "    email = email_match.group(1).strip() if email_match else None\n",
        "    address = address_match.group(1).strip() if address_match else None\n",
        "\n",
        "    return name, email, address\n",
        "\n",
        "# Corrected path using double backslashes or a raw string\n",
        "mypdf = PdfReader(\"/content/Vikashdraftresume.pdf\")\n",
        "\n",
        "# Extract text from the first page of the PDF\n",
        "page = mypdf.pages[0]\n",
        "text = page.extract_text()\n",
        "\n",
        "# Extract information from the PDF text\n",
        "name, email, address = extract_information(text)\n",
        "\n",
        "# Print the extracted information\n",
        "print(f\"Name: {name}\")\n",
        "print(f\"Email: {email}\")\n",
        "print(f\"Address: {address}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQyJAzpEYvUR",
        "outputId": "09f1749e-67b7-46f8-a483-39538e2f58a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: None\n",
            "Email: None\n",
            "Address: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web scraping"
      ],
      "metadata": {
        "id": "JfC06LZcbUGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://www.accuweather.com/en/in/noida/3146227/weather-forecast/3146227?city=noida\""
      ],
      "metadata": {
        "id": "wSK0C_LeZKh0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
      ],
      "metadata": {
        "id": "8JCNkki3boWC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "-RVRpDshcE_T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage=requests.get(url,headers=headers).text"
      ],
      "metadata": {
        "id": "mjC5vHHXbt8a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 #Impoting the libray beautiful soup"
      ],
      "metadata": {
        "id": "ki2oP1jfb4gp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = bs4.BeautifulSoup(webpage,\"lxml\") # Displaying the code in lxml format"
      ],
      "metadata": {
        "id": "OMAGCQ6QcRFx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = soup.find_all(\"div\",class_=\"real-feel\")"
      ],
      "metadata": {
        "id": "doA9GegDcT04"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq2SI0picbM8",
        "outputId": "35fd57a0-e37d-40c4-b0f1-98d52c69f9b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<div class=\"real-feel\">\n",
              " \t\t\t\t\t\tRealFeel®\n",
              " \t\t\t\t\t\t61°\n",
              " \t\t\t\t\t</div>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "html_content = '<div class=\"real-feel\">RealFeel® 18°</div>'\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "w9_sWc0hda82"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONh-Xj5efMdd",
        "outputId": "4c83139a-125f-4772-9994-6ca7f8749f84"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<div class=\"real-feel\">RealFeel® 18°</div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5) Language detect"
      ],
      "metadata": {
        "id": "QilXM0RhLUaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1 , using langid"
      ],
      "metadata": {
        "id": "EeNh-PMXR-UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXGl0VCrLiQY",
        "outputId": "bd88303c-0983-4320-fa04-35e3c62960db"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=a74c14d83a452c337bb7138e6c95cc1546bb10e855ec655165b9e365bd734e53\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "\n",
        "def language_detection(text):\n",
        "\n",
        "    # Use langid.py to detect the language\n",
        "    lang, _ = langid.classify(text)\n",
        "\n",
        "    return lang\n",
        "\n",
        "\n",
        "text_snippet = \"I am trying to detect this language.\" ## Example\n",
        "detected_language = language_detection(text_snippet)\n",
        "print(f\"Detected language:\", detected_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wIh_kzhh8sz",
        "outputId": "1223f36b-ea46-48c6-ea2f-4f8fd5df114d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_snippet_french = \"Je suis en train de détecter cette langue.\"\n",
        "detected_language_french = language_detection(text_snippet_french)\n",
        "print(f\"Detected language: {detected_language_french}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwkW2G0kLedW",
        "outputId": "8b687077-0062-4a77-bc80-78c8bc719833"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "\n",
        "def language_detection(text):\n",
        "\n",
        "    # Use langid.py to detect the language\n",
        "    lang, _ = langid.classify(text)\n",
        "\n",
        "    return lang,_\n",
        "\n",
        "\n",
        "text_snippet = \"I am trying to detect this language.\" ## Example\n",
        "detected_language = language_detection(text_snippet)"
      ],
      "metadata": {
        "id": "djjhH41yL6gF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detected_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY85zk36OjcA",
        "outputId": "ea64d820-c4e8-4f70-bb1f-06016c5d6d5c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('en', -99.13049244880676)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2"
      ],
      "metadata": {
        "id": "ezWTdIfcPMeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Langdetect"
      ],
      "metadata": {
        "id": "nVeRjacoSC2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmnegOjhQR1m",
        "outputId": "1b42e1e4-1394-49c6-eaa1-b9d7fca6cf26"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/981.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=1c2eff127cf9c19016790f3d26a5543b6d8149ef19efd90e44cbdf1087c27b8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "\n",
        "def language_detection_langdetect(text):\n",
        "    try:\n",
        "        # Use langdetect to detect the language\n",
        "        lang = detect(text)\n",
        "        return lang\n",
        "    except Exception as e:\n",
        "        # Handle exceptions (e.g., when langdetect cannot determine the language)\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "text_snippet = \"I am trying to detect this language.\"  # Example in English\n",
        "detected_language = language_detection_langdetect(text_snippet)\n",
        "print(f\"Detected language: {detected_language}\")\n",
        "\n",
        "text_snippet_french = \"Je suis en train de détecter cette langue.\"  # Example in French\n",
        "detected_language_french = language_detection_langdetect(text_snippet_french)\n",
        "print(f\"Detected language: {detected_language_french}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olTGAJf6Ol3s",
        "outputId": "805d62e7-cf84-4ead-94d8-686a1bc40c76"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "Detected language: fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using textblob"
      ],
      "metadata": {
        "id": "QbJIViRWSGAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WAHVopISNff",
        "outputId": "b0781734-e283-447c-b362-7066fa9a38fb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def language_detection_textblob(text):\n",
        "    try:\n",
        "        # Use TextBlob to detect the language\n",
        "        blob = TextBlob(text)\n",
        "        lang = blob.detect_language()\n",
        "        return lang\n",
        "    except Exception as e:\n",
        "        # Handle exceptions (e.g., when TextBlob cannot determine the language)\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "text_snippet = \"I am trying to detect this language.\"  # Example in English\n",
        "detected_language = language_detection_textblob(text_snippet)\n",
        "print(f\"Detected language: {detected_language}\")\n",
        "\n",
        "text_snippet_french = \"Je suis en train de détecter cette langue.\"  # Example in French\n",
        "detected_language_french = language_detection_textblob(text_snippet_french)\n",
        "print(f\"Detected language: {detected_language_french}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBo-RRYJQQHr",
        "outputId": "15828f30-34fb-4508-e756-9c781070d876"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: HTTP Error 400: Bad Request\n",
            "Detected language: None\n",
            "Error: HTTP Error 400: Bad Request\n",
            "Detected language: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) hosting your code on port 8091\n",
        "\n"
      ],
      "metadata": {
        "id": "VSGV-YIQUPAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from http.server import SimpleHTTPRequestHandler, HTTPServer\n",
        "class MyHandler(SimpleHTTPRequestHandler):\n",
        "    def do_GET(self):\n",
        "        self.send_response(200)\n",
        "        self.send_header('Content-type', 'text/html')\n",
        "        self.end_headers()\n",
        "        self.wfile.write(b'Hello, this is your server!')\n",
        "\n",
        "def run(server_class=HTTPServer, handler_class=MyHandler, port=8091):\n",
        "    server_address = ('', port)\n",
        "    httpd = server_class(server_address, handler_class)\n",
        "    print(f\"Starting server on port {port}...\")\n",
        "    httpd.serve_forever()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "kN1qeGwYSdPD",
        "outputId": "a946900a-af2a-4e29-a262-3d5ca040f0d4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server on port 8091...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-ec46a062a946>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-ec46a062a946>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(server_class, handler_class, port)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhttpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting server on port {port}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhttpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socketserver.py\u001b[0m in \u001b[0;36mserve_forever\u001b[0;34m(self, poll_interval)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__shutdown_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;31m# bpo-35017: shutdown() called during select(), exit immediately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__shutdown_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLZvtpgqUazB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}